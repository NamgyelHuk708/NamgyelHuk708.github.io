---
Title: DAM101 
categories: [DAM101, Unit4]
tags: [DAM101]
---

# Topic : Hyperparameter Tuning, Regularisation and Optimization
---
## Hyperparameter Tuning

![visual representation](<../visual hypertuning.png>)

### What is hyperparameter tuning?

When you’re training machine learning models, each dataset and model needs a different set of hyperparameters, which are a kind of variable. The only way to determine these is through multiple experiments, where you pick a set of hyperparameters and run them through your model. This is called hyperparameter tuning

### What is and some of the examples of hyperparameters?
Parameters that are used to control the behaviour of the model/algorithm and adjustable in order to obtain an improvised model with optimal performance is so-called **Hyperparameters**.

Examples of hyperparameters include the **number of nodes and layers in a neural network and the number of branches in a decision tree**. Hyperparameters determine **key features such as model architecture, learning rate, and model complexity**.

### What are some of hyperparameters?

- **Learning rate** is the rate at which an algorithm updates estimates
- **Learning rate decay** is a gradual reduction in the learning rate over time to speed up learning
- **Momentum** is the direction of the next step with respect to the previous step
- **Neural network nodes** refers to the number of nodes in each hidden layer
- **Neural network layers** refers to the number of hidden layers in a neural network
- **Number of hidden units/ neurons** in each layer
- **Mini-batch size** is training data batch size
- **Epochs** is the number of times the entire training dataset is shown to the network during training
- **Eta** is step size shrinkage to prevent overfitting.

### Why is hyperparameter tuning important?
Hyperparameters directly control model structure, function, and performance. Hence to have a higher accuurate model tuning a hyperparameter is important. A good and balanced choice of hyperparameters results in accurate models and excellent model performance.


### How do you identify hyperparameters?
There are no set rules on which hyperparameters work best nor their optimal or default values. You need to experiment to find the optimum hyperparameter set. This activity is known as hyperparameter tuning or hyperparameter optimization.

### What are the hyperparameter tuning techniques?
Most commonly used types:
- **Bayesian optimization**

Bayesian optimization is a technique based on Bayes’ theorem, which describes the probability of an event occurring related to current knowledge. When this is applied to hyperparameter optimization, the algorithm builds a probabilistic model from a set of hyperparameters that optimizes a specific metric. It uses regression analysis to iteratively choose the best set of hyperparameters.
![alt text](<../bayesian optimization.png>)

- **Grid search**

With grid search, you specify a list of hyperparameters and a performance metric, and the algorithm works through all possible combinations to determine the best fit

- **Random search**

Although based on similar principles as grid search, random search selects groups of hyperparameters randomly on each iteration. It works well when a relatively small number of the hyperparameters primarily determine the model outcome.
![alt text](<../grid search and random.png>)

## Hyperparameter Regularisation

### What is regularisation
Regularization is one of the most important concepts of machine learning. It is a technique to prevent the model from overfitting by adding extra information to it.

### Why do we do hyperparameter Regularisation
Sometimes the machine learning model performs well with the training data but does not perform well with the test data. It means the model is not able to predict the output when deals with unseen data by introducing noise in the output, and hence the model is called **overfitted**. This problem can be deal with the help of a **regularization technique**.

In simple to tackle overfitting

### Bias and Variance?
![alt text](<../hyperemeter regularisation.png>)

- first driagram: This is an example of **under-fitting ( high bias)** because the linear plane cannot capture the underlying trend of the input data.

- last diagram: In the diagram, we see a curve which does a great job at separating the 0’s from the X’s. However this does the job too well to cause **overfitting ( high variance)**. 

- middle diagram: The diagram shows a median plane which captures most of the data distribution without doing too much or too little. This is a case of **neither high bias nor high variance**.

### Techniques of Regularization
1. Ridge Regression
- Is mostly used to reduce the overfitting in the model, and it includes all the features present in the model. It reduces the complexity of the model by shrinking the coefficients.
- Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.
- Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.
- In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.
- The equation
![alt text](<../regularization tuyp1.png>)

2. Lasso Regression
- helps to reduce the overfitting in the model as well as feature selection.
- It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.
- Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.
- It is also called as L1 regularization. The equation for the cost function of Lasso regression will be
![alt text](../regularization-type2.png)

